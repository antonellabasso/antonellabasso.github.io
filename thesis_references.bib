@article{altmanHarmReductionFrameworkAlgorithmic2018,
	abstract = {In this article, we recognize the profound effects that algorithmic decision making can have on people's lives and propose a harm-reduction framework for algorithmic fairness. We argue that any evaluation of algorithmic fairness must take into account the foreseeable effects that algorithmic design, implementation, and use have on the well-being of individuals. We further demonstrate how counterfactual frameworks for causal inference developed in statistics and computer science can be used as the basis for defining and estimating the foreseeable effects of algorithmic decisions. Finally, we argue that certain patterns of foreseeable harms are unfair. An algorithmic decision is unfair if it imposes predictable harms on sets of individuals that are unconscionably disproportionate to the benefits these same decisions produce elsewhere. Also, an algorithmic decision is unfair when it is regressive, that is, when members of disadvantaged groups pay a higher cost for the social benefits of that decision.},
	author = {Altman, Micah and Wood, Alexandra and Vayena, Effy},
	doi = {10.1109/MSP.2018.2701149},
	file = {/Users/antonellabasso/Zotero/storage/X6V93UHS/Altman et al. - 2018 - A Harm-Reduction Framework for Algorithmic Fairnes.pdf;/Users/antonellabasso/Zotero/storage/SEW3LI6D/8395114.html},
	issn = {1558-4046},
	journal = {IEEE Security \& Privacy},
	keywords = {accountability,AI Ethics,Algorithm design and analysis,Artificial intelligence,Classification algorithms,data privacy,Decision making,Ethics,fairness,Inference algorithms,informational harm,Law,Machine learning algorithms,Prediction algorithms,privacy,Risk management},
	month = may,
	number = {3},
	pages = {34--45},
	title = {A {{Harm-Reduction Framework}} for {{Algorithmic Fairness}}},
	volume = {16},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/MSP.2018.2701149}}

@inproceedings{barocasFairnessMachineLearning2018,
	abstract = {Semantic Scholar extracted view of "Fairness and Machine Learning Limitations and Opportunities" by Solon Barocas et al.},
	author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
	file = {/Users/antonellabasso/Zotero/storage/5E22APY8/Barocas et al. - 2018 - Fairness and Machine Learning Limitations and Oppo.pdf},
	title = {Fairness and {{Machine Learning Limitations}} and {{Opportunities}}},
	urldate = {2023-02-24},
	year = {2018}}

@article{bonchiExposingProbabilisticCausal2017,
	abstract = {Discrimination discovery from data is an important task aiming at identifying patterns of illegal and unethical discriminatory activities against protected-by-law groups, e.g., ethnic minorities. While any legally-valid proof of discrimination requires evidence of causality, the state-of-the-art methods are essentially correlation-based, albeit, as it is well known, correlation does not imply causation. In this paper we take a principled causal approach to the data mining problem of discrimination detection in databases. Following Suppes' probabilistic causation theory, we define a method to extract, from a dataset of historical decision records, the causal structures existing among the attributes in the data. The result is a type of constrained Bayesian network, which we dub Suppes-Bayes Causal Network (SBCN). Next, we develop a toolkit of methods based on random walks on top of the SBCN, addressing different anti-discrimination legal concepts, such as direct and indirect discrimination, group and individual discrimination, genuine requirement, and favoritism. Our experiments on real-world datasets confirm the inferential power of our approach in all these different tasks.},
	author = {Bonchi, Francesco and Hajian, Sara and Mishra, Bud and Ramazzotti, Daniele},
	doi = {10.1007/s41060-016-0040-z},
	file = {/Users/antonellabasso/Zotero/storage/7JI3RNNK/Bonchi et al. - 2017 - Exposing the Probabilistic Causal Structure of Dis.pdf},
	journal = {International Journal of Data Science and Analytics},
	month = feb,
	title = {Exposing the {{Probabilistic Causal Structure}} of {{Discrimination}}},
	volume = {3},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1007/s41060-016-0040-z}}

@inproceedings{caldersControllingAttributeEffect2013,
	abstract = {In data mining we often have to learn from biased data, because, for instance, data comes from different batches or there was a gender or racial bias in the collection of social data. In some applications it may be necessary to explicitly control this bias in the models we learn from the data. This paper is the first to study learning linear regression models under constraints that control the biasing effect of a given attribute such as gender or batch number. We show how propensity modeling can be used for factoring out the part of the bias that can be justified by externally provided explanatory attributes. Then we analytically derive linear models that minimize squared error while controlling the bias by imposing constraints on the mean outcome or residuals of the models. Experiments with discrimination-aware crime prediction and batch effect normalization tasks show that the proposed techniques are successful in controlling attribute effects in linear regression models.},
	author = {Calders, Toon and Karim, Asim and Kamiran, Faisal and Ali, Wasif and Zhang, Xiangliang},
	booktitle = {2013 {{IEEE}} 13th {{International Conference}} on {{Data Mining}}},
	doi = {10.1109/ICDM.2013.114},
	file = {/Users/antonellabasso/Zotero/storage/7GNG3DRQ/Calders et al. - 2013 - Controlling Attribute Effect in Linear Regression.pdf;/Users/antonellabasso/Zotero/storage/BEVKM7BG/6729491.html},
	issn = {2374-8486},
	keywords = {Analytical models,Batch Effects,Biological system modeling,Data mining,Data models,Fair Data Mining,Linear regression,Linear Regression,Predictive models,Propensity Score,Vectors},
	month = dec,
	pages = {71--80},
	title = {Controlling {{Attribute Effect}} in {{Linear Regression}}},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/ICDM.2013.114}}

@misc{helwegenImprovingFairPredictions2020,
	abstract = {The importance of algorithmic fairness grows with the increasing impact machine learning has on people's lives. Recent work on fairness metrics shows the need for causal reasoning in fairness constraints. In this work, a practical method named FairTrade is proposed for creating flexible prediction models which integrate fairness constraints on sensitive causal paths. The method uses recent advances in variational inference in order to account for unobserved confounders. Further, a method outline is proposed which uses the causal mechanism estimates to audit black box models. Experiments are conducted on simulated data and on a real dataset in the context of detecting unlawful social welfare. This research aims to contribute to machine learning techniques which honour our ethical and legal boundaries.},
	archiveprefix = {arxiv},
	author = {Helwegen, Rik and Louizos, Christos and Forr{\'e}, Patrick},
	doi = {10.48550/arXiv.2008.10880},
	eprint = {arXiv:2008.10880},
	file = {/Users/antonellabasso/Zotero/storage/WZXXM9NX/Helwegen et al. - 2020 - Improving Fair Predictions Using Variational Infer.pdf;/Users/antonellabasso/Zotero/storage/NGGVTIIB/2008.html},
	keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
	month = aug,
	number = {arXiv:2008.10880},
	publisher = {{arXiv}},
	title = {Improving {{Fair Predictions Using Variational Inference In Causal Models}}},
	urldate = {2023-01-13},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2008.10880}}

@book{hernanCausalInferenceWhat2020,
	author = {Hernan, Miguel A and Robins, James M},
	file = {/Users/antonellabasso/Zotero/storage/RSIYSNAX/Hernan and Robins - Causal Inference What If.pdf},
	langid = {english},
	month = dec,
	publisher = {{Boca Raton: Chapman \& Hall/CRC.}},
	title = {Causal {{Inference}}: {{What If}}},
	year = {2020}}

@article{howeRecommendationsUsingCausal2022,
	abstract = {There have been calls for race to be denounced as a biological variable and for a greater focus on racism, instead of solely race, when studying racial health disparities in the United States. These calls are grounded in extensive scholarship and the rationale that race is not a biological variable, but instead socially constructed, and that structural/institutional racism is a root cause of race-related health disparities. However, there remains a lack of clear guidance for how best to incorporate these assertions about race and racism into tools, such as causal diagrams, that are commonly used by epidemiologists to study population health. We provide clear recommendations for using causal diagrams to study racial health disparities that were informed by these calls. These recommendations consider a health disparity to be a difference in a health outcome that is related to social, environmental, or economic disadvantage. We present simplified causal diagrams to illustrate how to implement our recommendations. These diagrams can be modified based on the health outcome and hypotheses, or for other group-based differences in health also rooted in disadvantage (e.g., gender). Implementing our recommendations may lead to the publication of more rigorous and informative studies of racial health disparities.},
	author = {Howe, Chanelle J and Bailey, Zinzi D and Raifman, Julia R and Jackson, John W},
	doi = {10.1093/aje/kwac140},
	file = {/Users/antonellabasso/Zotero/storage/6XCQ45V7/6653177.html},
	issn = {0002-9262},
	journal = {American Journal of Epidemiology},
	month = nov,
	number = {12},
	pages = {1981--1989},
	title = {Recommendations for {{Using Causal Diagrams}} to {{Study Racial Health Disparities}}},
	urldate = {2023-01-13},
	volume = {191},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1093/aje/kwac140}}

@article{issakohler-hausmannEddieMurphyDangers2019,
	author = {{Issa Kohler-Hausmann}},
	file = {/Users/antonellabasso/Zotero/storage/MFKPF8YJ/6.html},
	issn = {0029-3571},
	journal = {Northwestern University Law Review},
	month = mar,
	number = {5},
	pages = {1163--1228},
	title = {Eddie {{Murphy}} and the {{Dangers}} of {{Counterfactual Causal Thinking About Detecting Racial Discrimination}}},
	volume = {113},
	year = {2019}}

@article{kamiranDataPreprocessingTechniques2012,
	abstract = {Recently, the following Discrimination-Aware Classification Problem was introduced: Suppose we are given training data that exhibit unlawful discrimination; e.g., toward sensitive attributes such as gender or ethnicity. The task is to learn a classifier that optimizes accuracy, but does not have this discrimination in its predictions on test data. This problem is relevant in many settings, such as when the data are generated by a biased decision process or when the sensitive attribute serves as a proxy for unobserved features. In this paper, we concentrate on the case with only one binary sensitive attribute and a two-class classification problem. We first study the theoretically optimal trade-off between accuracy and non-discrimination for pure classifiers. Then, we look at algorithmic solutions that preprocess the data to remove discrimination before a classifier is learned. We survey and extend our existing data preprocessing techniques, being suppression of the sensitive attribute, massaging the dataset by changing class labels, and reweighing or resampling the data to remove discrimination without relabeling instances. These preprocessing techniques have been implemented in a modified version of Weka and we present the results of experiments on real-life data.},
	author = {Kamiran, Faisal and Calders, Toon},
	doi = {10.1007/s10115-011-0463-8},
	file = {/Users/antonellabasso/Zotero/storage/BNRWCW3T/Kamiran and Calders - 2012 - Data preprocessing techniques for classification w.pdf},
	issn = {0219-3116},
	journal = {Knowledge and Information Systems},
	keywords = {Classification,Discrimination-aware data mining,Preprocessing},
	langid = {english},
	month = oct,
	number = {1},
	pages = {1--33},
	title = {Data Preprocessing Techniques for Classification without Discrimination},
	urldate = {2023-01-13},
	volume = {33},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1007/s10115-011-0463-8}}

@inproceedings{kamishimaFairnessAwareClassifierPrejudice2012,
	abstract = {With the spread of data mining technologies and the accumulation of social data, such technologies and data are being used for determinations that seriously affect individuals' lives. For example, credit scoring is frequently determined based on the records of past credit data together with statistical prediction techniques. Needless to say, such determinations must be nondiscriminatory and fair in sensitive features, such as race, gender, religion, and so on. Several researchers have recently begun to attempt the development of analysis techniques that are aware of social fairness or discrimination. They have shown that simply avoiding the use of sensitive features is insufficient for eliminating biases in determinations, due to the indirect influence of sensitive information. In this paper, we first discuss three causes of unfairness in machine learning. We then propose a regularization approach that is applicable to any prediction algorithm with probabilistic discriminative models. We further apply this approach to logistic regression and empirically show its effectiveness and efficiency.},
	address = {{Berlin, Heidelberg}},
	author = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
	booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
	doi = {10.1007/978-3-642-33486-3_3},
	editor = {Flach, Peter A. and De Bie, Tijl and Cristianini, Nello},
	file = {/Users/antonellabasso/Zotero/storage/MEN4XIJP/Kamishima et al. - 2012 - Fairness-Aware Classifier with Prejudice Remover R.pdf},
	isbn = {978-3-642-33486-3},
	keywords = {classification,discrimination,fairness,information theory,logistic regression,social responsibility},
	langid = {english},
	pages = {35--50},
	publisher = {{Springer}},
	series = {Lecture {{Notes}} in {{Computer Science}}},
	title = {Fairness-{{Aware Classifier}} with {{Prejudice Remover Regularizer}}},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-642-33486-3_3}}

@misc{kilbertusSensitivityCounterfactualFairness2019,
	abstract = {Causal approaches to fairness have seen substantial recent interest, both from the machine learning community and from wider parties interested in ethical prediction algorithms. In no small part, this has been due to the fact that causal models allow one to simultaneously leverage data and expert knowledge to remove discriminatory effects from predictions. However, one of the primary assumptions in causal modeling is that you know the causal graph. This introduces a new opportunity for bias, caused by misspecifying the causal model. One common way for misspecification to occur is via unmeasured confounding: the true causal effect between variables is partially described by unobserved quantities. In this work we design tools to assess the sensitivity of fairness measures to this confounding for the popular class of non-linear additive noise models (ANMs). Specifically, we give a procedure for computing the maximum difference between two counterfactually fair predictors, where one has become biased due to confounding. For the case of bivariate confounding our technique can be swiftly computed via a sequence of closed-form updates. For multivariate confounding we give an algorithm that can be efficiently solved via automatic differentiation. We demonstrate our new sensitivity analysis tools in real-world fairness scenarios to assess the bias arising from confounding.},
	archiveprefix = {arxiv},
	author = {Kilbertus, Niki and Ball, Philip J. and Kusner, Matt J. and Weller, Adrian and Silva, Ricardo},
	doi = {10.48550/arXiv.1907.01040},
	eprint = {arXiv:1907.01040},
	file = {/Users/antonellabasso/Zotero/storage/7Z4RD7AD/Kilbertus et al. - 2019 - The Sensitivity of Counterfactual Fairness to Unme.pdf;/Users/antonellabasso/Zotero/storage/7FA4CLF8/1907.html},
	keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
	month = jul,
	number = {arXiv:1907.01040},
	publisher = {{arXiv}},
	title = {The {{Sensitivity}} of {{Counterfactual Fairness}} to {{Unmeasured Confounding}}},
	urldate = {2023-01-13},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1907.01040}}

@article{langeAssessingNaturalDirect2014,
	abstract = {Within the fields of epidemiology, interventions research and social sciences researchers are often faced with the challenge of decomposing the effect of an exposure into different causal pathways working through defined mediator variables. The goal of such analyses is often to understand the mechanisms of the system or to suggest possible interventions. The case of a single mediator, thus implying only 2 causal pathways (direct and indirect) from exposure to outcome, has been extensively studied. By using the framework of counterfactual variables, researchers have established theoretical properties and developed powerful tools. However, in practical problems, it is not uncommon to have several distinct causal pathways from exposure to outcome operating through different mediators. In this article, we suggest a widely applicable approach to quantifying and ranking different causal pathways. The approach is an extension of the natural effect models proposed by Lange et al. (Am J Epidemiol. 2012;176(3):190\textendash 195). By allowing the analysis of distinct multiple pathways, the suggested approach adds to the capabilities of modern mediation techniques. Furthermore, the approach can be implemented using standard software, and we have included with this article implementation examples using R (R Foundation for Statistical Computing, Vienna, Austria) and Stata software (StataCorp LP, College Station, Texas).},
	author = {Lange, Theis and Rasmussen, Mette and Thygesen, Lau Caspar},
	doi = {10.1093/aje/kwt270},
	file = {/Users/antonellabasso/Zotero/storage/ZRYBPWHG/Lange et al. - 2014 - Assessing Natural Direct and Indirect Effects Thro.pdf;/Users/antonellabasso/Zotero/storage/NVZESMD6/128034.html},
	issn = {0002-9262},
	journal = {American Journal of Epidemiology},
	month = feb,
	number = {4},
	pages = {513--518},
	title = {Assessing {{Natural Direct}} and {{Indirect Effects Through Multiple Pathways}}},
	urldate = {2023-04-24},
	volume = {179},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1093/aje/kwt270}}

@misc{loftusCausalReasoningAlgorithmic2018,
	abstract = {In this work, we argue for the importance of causal reasoning in creating fair algorithms for decision making. We give a review of existing approaches to fairness, describe work in causality necessary for the understanding of causal approaches, argue why causality is necessary for any approach that wishes to be fair, and give a detailed analysis of the many recent approaches to causality-based fairness.},
	archiveprefix = {arxiv},
	author = {Loftus, Joshua R. and Russell, Chris and Kusner, Matt J. and Silva, Ricardo},
	doi = {10.48550/arXiv.1805.05859},
	eprint = {arXiv:1805.05859},
	file = {/Users/antonellabasso/Zotero/storage/ND5RSEBU/Loftus et al. - 2018 - Causal Reasoning for Algorithmic Fairness.pdf;/Users/antonellabasso/Zotero/storage/V49JB5NU/1805.html},
	keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
	month = may,
	number = {arXiv:1805.05859},
	publisher = {{arXiv}},
	title = {Causal {{Reasoning}} for {{Algorithmic Fairness}}},
	urldate = {2023-01-13},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1805.05859}}

@misc{loftusCounterfactualFairness2018,
	abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
	archiveprefix = {arxiv},
	author = {Loftus, Joshua R. and Kusner, Matt J. and Russell, Chris and Silva, Ricardo},
	doi = {10.48550/arXiv.1703.06856},
	eprint = {arXiv:1703.06856},
	file = {/Users/antonellabasso/Zotero/storage/HNC5QKZW/Kusner et al. - 2018 - Counterfactual Fairness.pdf;/Users/antonellabasso/Zotero/storage/NYTBVVVF/1703.html},
	keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
	month = mar,
	number = {arXiv:1703.06856},
	publisher = {{arXiv}},
	title = {Counterfactual {{Fairness}}},
	urldate = {2023-01-13},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1703.06856}}

@article{mancuhanCombatingDiscriminationUsing2014,
	abstract = {Discrimination in decision making is prohibited on many attributes (religion, gender, etc\ldots ), but often present in historical decisions. Use of such discriminatory historical decision making as training data can perpetuate discrimination, even if the protected attributes are not directly present in the data. This work focuses on discovering discrimination in instances and preventing discrimination in classification. First, we propose a discrimination discovery method based on modeling the probability distribution of a class using Bayesian networks. This measures the effect of a protected attribute (e.g., gender) in a subset of the dataset using the estimated probability distribution (via a Bayesian network). Second, we propose a classification method that corrects for the discovered discrimination without using protected attributes in the decision process. We evaluate the discrimination discovery and discrimination prevention approaches on two different datasets. The empirical results show that a substantial amount of discrimination identified in instances is prevented in future decisions.},
	author = {Mancuhan, Koray and Clifton, Chris},
	doi = {10.1007/s10506-014-9156-4},
	issn = {1572-8382},
	journal = {Artificial Intelligence and Law},
	keywords = {Bayesian network,Data mining,Discrimination discovery,Discrimination prevention},
	langid = {english},
	month = jun,
	number = {2},
	pages = {211--238},
	title = {Combating Discrimination Using {{Bayesian}} Networks},
	urldate = {2023-01-13},
	volume = {22},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1007/s10506-014-9156-4}}

@misc{mehrabiSurveyBiasFairness2022,
	abstract = {With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
	archiveprefix = {arxiv},
	author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	doi = {10.48550/arXiv.1908.09635},
	eprint = {arXiv:1908.09635},
	file = {/Users/antonellabasso/Zotero/storage/UMUJHYX9/Mehrabi et al. - 2022 - A Survey on Bias and Fairness in Machine Learning.pdf;/Users/antonellabasso/Zotero/storage/LW7JJRYL/1908.html},
	keywords = {Computer Science - Machine Learning},
	month = jan,
	number = {arXiv:1908.09635},
	publisher = {{arXiv}},
	title = {A {{Survey}} on {{Bias}} and {{Fairness}} in {{Machine Learning}}},
	urldate = {2023-01-13},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1908.09635}}

@misc{mehrabiAttributingFairDecisions2021,
	abstract = {The widespread use of Artificial Intelligence (AI) in consequential domains, such as healthcare and parole decision-making systems, has drawn intense scrutiny on the fairness of these methods. However, ensuring fairness is often insufficient as the rationale for a contentious decision needs to be audited, understood, and defended. We propose that the attention mechanism can be used to ensure fair outcomes while simultaneously providing feature attributions to account for how a decision was made. Toward this goal, we design an attention-based model that can be leveraged as an attribution framework. It can identify features responsible for both performance and fairness of the model through attention interventions and attention weight manipulation. Using this attribution framework, we then design a post-processing bias mitigation strategy and compare it with a suite of baselines. We demonstrate the versatility of our approach by conducting experiments on two distinct data types, tabular and textual.},
	archiveprefix = {arxiv},
	author = {Mehrabi, Ninareh and Gupta, Umang and Morstatter, Fred and Steeg, Greg Ver and Galstyan, Aram},
	doi = {10.48550/arXiv.2109.03952},
	eprint = {arXiv:2109.03952},
	file = {/Users/antonellabasso/Zotero/storage/W4ZUSFXS/Mehrabi et al. - 2021 - Attributing Fair Decisions with Attention Interven.pdf;/Users/antonellabasso/Zotero/storage/KFERLMXA/2109.html},
	keywords = {Computer Science - Artificial Intelligence},
	month = sep,
	number = {arXiv:2109.03952},
	publisher = {{arXiv}},
	title = {Attributing {{Fair Decisions}} with {{Attention Interventions}}},
	urldate = {2023-03-05},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2109.03952}}

@article{nabiFairInferenceOutcomes2017,
	abstract = {In this paper, we consider the problem of fair statistical inference involving outcome variables. Examples include classification and regression problems, and estimating treatment effects in randomized trials or observational data. The issue of fairness arises in such problems where some covariates or treatments are "sensitive," in the sense of having potential of creating discrimination. In this paper, we argue that the presence of discrimination can be formalized in a sensible way as the presence of an effect of a sensitive covariate on the outcome along certain causal pathways, a view which generalizes (Pearl, 2009). A fair outcome model can then be learned by solving a constrained optimization problem. We discuss a number of complications that arise in classical statistical inference due to this view and provide workarounds based on recent work in causal and semi-parametric inference.},
	author = {Nabi, Razieh and Shpitser, Ilya},
	doi = {10.48550/arXiv.1705.10378},
	file = {/Users/antonellabasso/Zotero/storage/9SPL9F36/Nabi and Shpitser - 2017 - Fair Inference On Outcomes.pdf},
	langid = {english},
	month = may,
	title = {Fair {{Inference On Outcomes}}},
	urldate = {2023-03-10},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1705.10378}}

@inproceedings{nabiOptimalTrainingFair2022,
	abstract = {Recently there has been sustained interest in modifying prediction algorithms to satisfy fairness constraints. These constraints are typically complex nonlinear functionals of the observed data distribution. Focusing on the path-specific causal constraints, we introduce new theoretical results and optimization techniques to make model training easier and more accurate. Specifically, we show how to reparameterize the observed data likelihood such that fairness constraints correspond directly to parameters that appear in the likelihood, transforming a complex constrained optimization objective into a simple optimization problem with box constraints. We also exploit methods from empirical likelihood theory in statistics to improve predictive performance by constraining baseline covariates, without requiring parametric models. We combine the merits of both proposals to optimize a hybrid reparameterized likelihood. The techniques presented here should be applicable more broadly to fair prediction proposals that impose constraints on predictive models.},
	author = {Nabi, Razieh and Malinsky, Daniel and Shpitser, Ilya},
	booktitle = {Proceedings of the {{First Conference}} on {{Causal Learning}} and {{Reasoning}}},
	file = {/Users/antonellabasso/Zotero/storage/S8HYUFP9/Nabi et al. - 2022 - Optimal Training of Fair Predictive Models.pdf},
	issn = {2640-3498},
	langid = {english},
	month = jun,
	pages = {594--617},
	publisher = {{PMLR}},
	title = {Optimal {{Training}} of {{Fair Predictive Models}}},
	urldate = {2023-01-13},
	year = {2022}}

@article{pearlCausalInferenceStatistics2009,
	abstract = {This review presents empirical researchers with recent advances in causal inference, and stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underly all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: (1) queries about the effects of potential interventions, (also called ``causal effects'' or ``policy evaluation'') (2) queries about probabilities of counterfactuals, (including assessment of ``regret,'' ``attribution'' or ``causes of effects'') and (3) queries about direct and indirect effects (also known as ``mediation''). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both.},
	author = {Pearl, Judea},
	doi = {10.1214/09-SS057},
	file = {/Users/antonellabasso/Zotero/storage/M38F92BE/Pearl - 2009 - Causal inference in statistics An overview.pdf},
	issn = {1935-7516},
	journal = {Statistics Surveys},
	langid = {english},
	month = jan,
	number = {none},
	shorttitle = {Causal Inference in Statistics},
	title = {Causal Inference in Statistics: {{An}} Overview},
	urldate = {2023-01-13},
	volume = {3},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1214/09-SS057}}

@techreport{pearlInterpretableConditionsIdentifying2012,
	abstract = {This paper translates the conditions necessary for the identification of natural direct and indirect effects into a transparent language, thus permitting a more informed judgment of the plausibility of these conditions. We show that the conditions usually cited in the literature are overly restricted, and can be relaxed substantially, without compromising identification. In particular, we show that natural effects can be identified by methods other than adjustment. The identification conditions can be further relaxed in parametric models with interactions, and permit us to compare the relative importance of several pathways, mediated by interdependent variables.},
	address = {{Fort Belvoir, VA}},
	author = {Pearl, Judea},
	doi = {10.21236/ADA564093},
	file = {/Users/antonellabasso/Zotero/storage/KSC4DQUE/Pearl - 2012 - Interpretable Conditions for Identifying Direct an.pdf},
	institution = {{Defense Technical Information Center}},
	langid = {english},
	month = jun,
	shorttitle = {Interpretable {{Conditions}} for {{Identifying Direct}} and {{Indirect Effects}}},
	title = {Interpretable {{Conditions}} for {{Identifying Direct}} and {{Indirect Effects}}:},
	urldate = {2023-03-10},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.21236/ADA564093}}

@article{pearlCausalMediationFormula2012,
	abstract = {Recent advances in causal inference have given rise to a general and easy-to-use formula for assessing the extent to which the effect of one variable on another is mediated by a third. This Mediation Formula is applicable to nonlinear models with both discrete and continuous variables, and permits the evaluation of path-specific effects with minimal assumptions regarding the data-generating process. We demonstrate the use of the Mediation Formula in simple examples and illustrate why parametric methods of analysis yield distorted results, even when parameters are known precisely. We stress the importance of distinguishing between the necessary and sufficient interpretations of ``mediated-effect'' and show how to estimate the two components in nonlinear systems with continuous and categorical variables.},
	author = {Pearl, Judea},
	doi = {10.1007/s11121-011-0270-1},
	file = {/Users/antonellabasso/Zotero/storage/DMURUSNZ/Pearl - 2012 - The Causal Mediation Formula---A Guide to the Assess.pdf},
	issn = {1573-6695},
	journal = {Prevention Science},
	keywords = {Direct and indirect effects,Effect decomposition,Moderation,Percentage explained,Structural equation models},
	langid = {english},
	month = aug,
	number = {4},
	pages = {426--436},
	title = {The {{Causal Mediation Formula}}\textemdash{{A Guide}} to the {{Assessment}} of {{Pathways}} and {{Mechanisms}}},
	urldate = {2023-03-10},
	volume = {13},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1007/s11121-011-0270-1}}

@misc{qureshiCausalInferenceSocial2019,
	abstract = {The discovery of discriminatory bias in human or automated decision making is a task of increasing importance and difficulty, exacerbated by the pervasive use of machine learning and data mining. Currently, discrimination discovery largely relies upon correlation analysis of decisions records, disregarding the impact of confounding biases. We present a method for causal discrimination discovery based on propensity score analysis, a statistical tool for filtering out the effect of confounding variables. We introduce causal measures of discrimination which quantify the effect of group membership on the decisions, and highlight causal discrimination/favoritism patterns by learning regression trees over the novel measures. We validate our approach on two real world datasets. Our proposed framework for causal discrimination has the potential to enhance the transparency of machine learning with tools for detecting discriminatory bias both in the training data and in the learning algorithms.},
	archiveprefix = {arxiv},
	author = {Qureshi, Bilal and Kamiran, Faisal and Karim, Asim and Ruggieri, Salvatore and Pedreschi, Dino},
	doi = {10.48550/arXiv.1608.03735},
	eprint = {arXiv:1608.03735},
	file = {/Users/antonellabasso/Zotero/storage/YJP3K4P4/Qureshi et al. - 2019 - Causal Inference for Social Discrimination Reasoni.pdf;/Users/antonellabasso/Zotero/storage/XLY7DJTE/1608.html},
	keywords = {Computer Science - Computers and Society,Statistics - Applications},
	month = nov,
	number = {arXiv:1608.03735},
	publisher = {{arXiv}},
	title = {Causal {{Inference}} for {{Social Discrimination Reasoning}}},
	urldate = {2023-01-14},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1608.03735}}

@misc{zhangMitigatingUnwantedBiases2018,
	abstract = {Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.},
	archiveprefix = {arxiv},
	author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
	doi = {10.48550/arXiv.1801.07593},
	eprint = {arXiv:1801.07593},
	file = {/Users/antonellabasso/Zotero/storage/63393V8S/Zhang et al. - 2018 - Mitigating Unwanted Biases with Adversarial Learni.pdf;/Users/antonellabasso/Zotero/storage/VLDF5AD4/1801.html},
	keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
	month = jan,
	number = {arXiv:1801.07593},
	publisher = {{arXiv}},
	title = {Mitigating {{Unwanted Biases}} with {{Adversarial Learning}}},
	urldate = {2023-03-05},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.1801.07593}}

@article{zhangCausalModelingBasedDiscrimination2019,
	abstract = {Anti-discrimination is an increasingly important task in data science. In this paper, we investigate the problem of discovering both direct and indirect discrimination from the historical data, and removing the discriminatory effects before the data are used for predictive analysis (e.g., building classifiers). The main drawback of existing methods is that they cannot distinguish the part of influence that is really caused by discrimination from all correlated influences. In our approach, we make use of the causal graph to capture the causal structure of the data. Then, we model direct and indirect discrimination as the path-specific effects, which accurately identify the two types of discrimination as the causal effects transmitted along different paths in the graph. For certain situations where indirect discrimination cannot be exactly measured due to the unidentifiability of some path-specific effects, we develop an upper bound and a lower bound to the effect of indirect discrimination. Based on the theoretical results, we propose effective algorithms for discovering direct and indirect discrimination, as well as algorithms for precisely removing both types of discrimination while retaining good data utility. Experiments using the real dataset show the effectiveness of our approaches.},
	author = {Zhang, Lu and Wu, Yongkai and Wu, Xintao},
	doi = {10.1109/TKDE.2018.2872988},
	file = {/Users/antonellabasso/Zotero/storage/M2BWL8QX/8477109.html},
	issn = {1558-2191},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	keywords = {causal modeling,Data models,direct and indirect discrimination,Discrimination discovery and removal,Law,Mathematical model,Measurement,path-specific effect,Prediction algorithms,Predictive models,Task analysis},
	month = nov,
	number = {11},
	pages = {2035--2050},
	shorttitle = {Causal {{Modeling-Based Discrimination Discovery}} and {{Removal}}},
	title = {Causal {{Modeling-Based Discrimination Discovery}} and {{Removal}}: {{Criteria}}, {{Bounds}}, and {{Algorithms}}},
	volume = {31},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TKDE.2018.2872988}}
